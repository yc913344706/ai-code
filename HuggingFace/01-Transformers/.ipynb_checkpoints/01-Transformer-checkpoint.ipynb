{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3334a096-df59-40a2-b156-eb4dd9afefbb",
   "metadata": {},
   "source": [
    "# 1 pipeline\n",
    "\n",
    "The pipeline function is the most high-level API of Transformers library.\n",
    "\n",
    "lt regroups together all the steps to go from raw texts to usable predictions. \n",
    "\n",
    "It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer.\n",
    "\n",
    "There are three main steps involved when you pass some text to a pipeline:\n",
    "\n",
    "- The text is preprocessed into a format the model can understand.\n",
    "- The preprocessed inputs are passed to the model.\n",
    "- The predictions of the model are post-processed, so you can make sense of them.\n",
    "\n",
    "Some of the currently available [pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) are:\n",
    "\n",
    "- feature-extraction (get the vector representation of a text)\n",
    "- fill-mask\n",
    "- ner (named entity recognition)\n",
    "- question-answering\n",
    "- sentiment-analysis\n",
    "- summarization\n",
    "- text-generation\n",
    "- translation\n",
    "- zero-shot-classification\n",
    "\n",
    "Let’s have a look at a few of these!\n",
    "\n",
    "## 1.1 sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a9299a2-4955-49f3-b32d-b4c3baade74d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 22:16:45.879990: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-19 22:16:45.893948: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-19 22:16:45.910213: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-19 22:16:45.915268: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-19 22:16:45.928766: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-19 22:16:46.883634: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18efaf5bf914f9c8b96adfc40ac9c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 22:17:11.848880: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245c8d34524a4bd1a6cd2dfbc59bef6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca0431a9b8148e29c1a57cd321e049b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598047137260437}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba491765-f5be-4cdc-b4a2-24a8d9a800b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e387d4-58fe-4afd-ad3a-b357e9606f91",
   "metadata": {},
   "source": [
    "# 1.2 zero-shot-classification\n",
    "\n",
    "classify texts that haven’t been labelled.\n",
    "\n",
    "This pipeline is called zero-shot because you don’t need to fine-tune the model on your data to use it. It can directly return probability scores for any list of labels you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ed55751-04e4-4a11-b4e7-1e99e3fb366a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to FacebookAI/roberta-large-mnli and revision 130fb28 (https://huggingface.co/FacebookAI/roberta-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.9562343955039978, 0.02697221003472805, 0.01679338701069355]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dd9c5e-81ec-4975-b02a-c8a87abfaff5",
   "metadata": {},
   "source": [
    "## 1.3 text generation\n",
    "\n",
    "### 1.3.1 text generation with default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12cb8fa7-8986-47c0-bb6c-09c68cd037b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f98a5cc42874232bfca2b1726a05113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c189f14dc11d44cfa1a817ac3622b836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a5245d649a4a5096a9262d5057868f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a153759e202b4d0e9c1aaaacbd442717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5f9a6fa5bb469c9fd4afaa8ed6f80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d9a19867bf41708b04c41e1a189aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to do the \"A-2/B\" trick. You will learn how to make more complex and unique combinations of the A-2/B moves. You will learn how to do the \"D'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"In this course, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4d092b-0de2-48ab-9121-4d75cf2f99a5",
   "metadata": {},
   "source": [
    "### 1.3.2 text generation with specific model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40744427-ad16-4c86-9776-ccc732f331fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17932f21906e4d4b9f13e4cc2ea149fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ec6bde4943488bbddcb71b71de4c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ed8f89fcbf4995a67af86053e0b2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c904d17d547f48bda50acc3fb20e9b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9981f29dc94cdda8c7f2f9a396b3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c55876595e45909bd43a40f48f70ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to practice how you can develop in a productive way. We believe you will learn how to use the data'},\n",
       " {'generated_text': 'In this course, we will teach you how to teach using the Android App Engine. After you do our lesson, we will put out on an informative'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0604376a-6f54-4b54-a21e-01aed68f9625",
   "metadata": {},
   "source": [
    "## 1.4 Inference API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babdf646-c066-4804-bf12-f661c89e00ce",
   "metadata": {},
   "source": [
    "## 1.5 Mask filling\n",
    "\n",
    "The top_k argument controls how many possibilities you want to be displayed. Note that here the model fills in the special <mask> word, which is often referred to as a mask token. Other mask-filling models might have different mask tokens, so it’s always good to verify the proper mask word when exploring other models. One way to check it is by looking at the mask word used in the widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc3fbd63-dff6-4fd8-905c-3ba8f1bae047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision ec58a5b (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c2291d0ab442fdb74091a7b1028f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695b79744bc84701bc602ce54c8bf0bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2911b21e6e4e0e9ef14477ddce6e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "907e914c861344ac8ee80aff578a9e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d2bd92dc97459fbf6609fd5a23777b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06233f071353467d9e90cc4173eeba14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.19619585573673248,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you all about mathematical models.'},\n",
       " {'score': 0.04052681848406792,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational',\n",
       "  'sequence': 'This course will teach you all about computational models.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf2f740-08e8-4817-8c6d-deb4b41f95a5",
   "metadata": {},
   "source": [
    "## 1.6 Named entity recognition (NER) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "798ee6eb-7bf5-4e5c-96ad-75e37c94a01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f181522d7b4e7291fc07a625bc98dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "decc6f95af4744729175b3ffa7e59463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4dd2741c1d4183a04d0f17e61014ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a4e2f9dcbf40edae97a070a64c48c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vayne/.e/jupyter/lib/python3.9/site-packages/transformers/pipelines/token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9981694,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9796019,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9932106,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba20c788-0918-40f2-89b7-ee17d6b601ac",
   "metadata": {},
   "source": [
    "Here the model correctly identified that Sylvain is a person (PER), Hugging Face an organization (ORG), and Brooklyn a location (LOC).\n",
    "\n",
    "We pass the option grouped_entities=True in the pipeline creation function to tell the pipeline to regroup together the parts of the sentence that correspond to the same entity: here the model correctly grouped “Hugging” and “Face” as a single organization, even though the name consists of multiple words. In fact, as we will see in the next chapter, the preprocessing even splits some words into smaller parts. For instance, Sylvain is split into four pieces: S, ##yl, ##va, and ##in. In the post-processing step, the pipeline successfully regrouped those pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c6607d-a08c-4a13-90f0-e13b60a614b3",
   "metadata": {},
   "source": [
    "## 1.7 question answering\n",
    "\n",
    "Note that this pipeline works by extracting information from the provided context; it does not generate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42529b96-d65e-472b-b319-90acefaf7936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3fdd7192c34c4380742b99c2e0b052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd786e3666b4b95b18966e2b6256e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3e07bac2984fef93aaf5834ecb3577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebac8f0cf1b645ab938d9b34a983259d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32d17f0bfe94b14b8a6814fd9b3200a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.6949754953384399, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74565bb7-3ac9-4b12-b5ca-fff267b603c8",
   "metadata": {},
   "source": [
    "## 1.8 Summarization\n",
    "\n",
    "Summarization is the task of reducing a text into a shorter text while keeping all (or most) of the important aspects referenced in the text. Here’s an example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edec5d7-f585-4e57-a23c-d2a79c7013fe",
   "metadata": {},
   "source": [
    "## 1.9 Translation\n",
    "\n",
    "For translation, you can use a default model if you provide a language pair in the task name (such as \"translation_en_to_fr\"), but the easiest way is to pick the model you want to use on the Model Hub. Here we’ll try translating from French to English:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7612e13-6801-45c6-a95d-bfccb10a89f1",
   "metadata": {},
   "source": [
    "# 2 Transformer\n",
    "\n",
    "## 2.1 what is Transformer\n",
    "\n",
    "The Transformer architecture was introduced in June 2017. The focus of the original research was on translation tasks. \n",
    "\n",
    "- Transformers are language models\n",
    "- Transformers are big models\n",
    "\n",
    "Imagine if each time a research team, a student organization, or a company wanted to train a model, it did so from scratch. This would lead to huge, unnecessary global costs!\n",
    "\n",
    "This is why sharing language models is paramount: sharing the trained weights and building on top of already trained weights reduces the overall compute cost and carbon footprint of the community.\n",
    "\n",
    "## 2.2 general architecture\n",
    "\n",
    "the general architecture of the Transformer model\n",
    "\n",
    "The model is primarily composed of two blocks:\n",
    "\n",
    "- Encoder (left): The encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire understanding from the input.\n",
    "- Decoder (right): The decoder uses the encoder’s representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs.\n",
    "\n",
    "Each of these parts can be used independently, depending on the task:\n",
    "\n",
    "- Encoder-only models:\n",
    "  - Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.\n",
    "  - Encoder models are best suited for tasks **requiring an understanding of the full sentence**, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering.\n",
    "  - These models are often characterized as having “bi-directional” attention, and are often called auto-encoding models.\n",
    "  - Representatives of this family of models include: `ALBERT`, `BERT`, `DistilBERT`, `ELECTRA`, `RoBERTa`.\n",
    "- Decoder-only models:\n",
    "  - Good for generative tasks such as text generation.\n",
    "  - These models are often called auto-regressive models.\n",
    "  - The pretraining of decoder models usually revolves around **predicting the next word in the sentence**.\n",
    "  - These models are best suited for tasks involving text generation.\n",
    "  - Representatives of this family of models: `CTRL`, `GPT`, `GPT-2`, `Transformer XL`.\n",
    "- Encoder-decoder models or sequence-to-sequence models:\n",
    "  - Good for generative tasks that require an input, such as translation or summarization.\n",
    "  - Sequence-to-sequence models are best suited for tasks revolving around **generating new sentences depending on a given input**, such as summarization, translation, or generative question answering.\n",
    "  - Representatives of this family of models include: `BART`, `mBART`, `Marian`, `T5`.\n",
    "\n",
    "|Model|\tExamples |\tTasks|\n",
    "|---|---|---|\n",
    "|Encoder | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa | Sentence classification, named entity recognition, extractive question answering |\n",
    "|Decoder | CTRL, GPT, GPT-2, Transformer XL |\tText generation |\n",
    "|Encoder-decoder | BART, T5, Marian, mBART |Summarization, translation, generative question answering |\n",
    "\n",
    "\n",
    "\n",
    "## 2.3 Attention Layer\n",
    "\n",
    "this layer will tell the model to pay specific attention to certain words in the sentence you passed it (and more or less ignore the others) when dealing with the representation of **each** word.\n",
    "\n",
    "## 2.4 original architecture\n",
    "\n",
    "The Transformer architecture was originally designed for translation.\n",
    "\n",
    "## 2.5 bias and limitation\n",
    "\n",
    "If your intent is to use a pretrained model or a fine-tuned version in production, please be aware that, while these models are powerful tools, they come with limitations.\n",
    "\n",
    "The biggest of these is that, to enable pretraining on large amounts of data, researchers often scrape all the content they can find, taking the best as well as the worst of what is available on the internet.\n",
    "\n",
    "When you use these tools, you therefore need to keep in the back of your mind that the original model you are using could very easily generate sexist, racist, or homophobic content. \n",
    "\n",
    "> Note: Fine-tuning the model on your data won’t make this intrinsic bias disappear.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880b4fa7-8bb8-4d0f-b246-a50e92ffc60e",
   "metadata": {},
   "source": [
    "# 3 detail in Transformer model\n",
    "\n",
    "## 3.1 Why Transformer models\n",
    "\n",
    "As you saw in Chapter 1, Transformer models are usually very large. With millions to tens of billions of parameters, training and deploying these models is a complicated undertaking. Furthermore, with new models being released on a near-daily basis and each having its own implementation, trying them all out is no easy task.\n",
    "\n",
    "The 🤗 Transformers library was created to solve this problem. Its goal is to provide a single API through which any Transformer model can be loaded, trained, and saved. \n",
    "\n",
    "## 3.2 detail in pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8885fc4-c3b5-45f9-983b-cdb37d9f5eea",
   "metadata": {},
   "source": [
    "### 3.2.1 tokenizer\n",
    "\n",
    "Like other neural networks, Transformer models can’t process raw text directly, so the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. To do this we use a tokenizer, which will be responsible for:\n",
    "\n",
    "- Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens\n",
    "- Mapping each token to an integer\n",
    "- Adding additional inputs that may be useful to the model\n",
    "\n",
    "All this preprocessing needs to be done in exactly the same way as when the model was pretrained, so we first need to download that information from the [Model Hub](https://huggingface.co/models). To do this, we use the AutoTokenizer class and its from_pretrained() method. \n",
    "\n",
    "This model ([distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english)) is a fine-tune checkpoint of [DistilBERT-base-uncased](https://huggingface.co/distilbert/distilbert-base-uncased).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4804552b-7fd1-4b32-abb4-282558352834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49d696976da46b798613c461ac18591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429e6c2614844667a651db595f1092ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272972f06a7e4b649566ff5520e8259b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c461c0-eb84-41d5-ac20-bf5bfcf622f6",
   "metadata": {},
   "source": [
    "Once we have the tokenizer, we can directly pass our sentences to it and we’ll get back a dictionary that’s ready to feed to our model! The only thing left to do is to convert the list of input IDs to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "994476be-be28-403e-9001-facd32a563cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
      "array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
      "        12172,  2607,  2026,  2878,  2166,  1012,   102],\n",
      "       [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b3ef59-62ea-4548-a0bb-d41b0a6c63cb",
   "metadata": {},
   "source": [
    "The output itself is a dictionary containing two keys, input_ids and attention_mask. \n",
    "\n",
    "- input_ids contains two rows of integers (one for each sentence) that are the unique identifiers of the tokens in each sentence.\n",
    "- We’ll explain what the attention_mask is later in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147e7842-1fd2-493e-8af8-3ca123914385",
   "metadata": {},
   "source": [
    "### 3.2.2 model\n",
    "\n",
    "We can download our pretrained model the same way we did with our tokenizer. 🤗 Transformers provides an AutoModel class which also has a from_pretrained() method.\n",
    "\n",
    "This architecture contains only the base Transformer module: given some inputs, it outputs what we’ll call hidden states, also known as features. \n",
    "\n",
    "ther are may models:\n",
    "\n",
    "* Model (retrieve the hidden states)\n",
    "* ForCausalLM\n",
    "* ForMaskedLM\n",
    "* ForMultipleChoice\n",
    "* ForQuestionAnswering\n",
    "* ForSequenceClassification\n",
    "* ForTokenClassification\n",
    "* and others 🤗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2e64f1d-1623-40e3-87eb-db32e1367223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c650c0cdf040d0b9bdebbbfb72476a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 16, 768)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = TFAutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "outputs = model(inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7db9b52d-7d41-4b57-8a41-dbe5944e08f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75357e2a-86aa-4a62-9d7f-d86d046e54a1",
   "metadata": {},
   "source": [
    "### 3.2.3 model output logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff77c447-fc56-426d-a130-33340ab7c8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-1.5606984  1.6122835]\n",
      " [ 4.16923   -3.3464472]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea550a20-18ac-41f0-8681-5f74a7d87330",
   "metadata": {},
   "source": [
    "Our model predicted [-1.5607, 1.6123] for the first sentence and [ 4.1692, -3.3464] for the second one. Those are not probabilities but logits, the raw, unnormalized scores outputted by the last layer of the model. \n",
    "\n",
    "To be converted to probabilities, they need to go through a SoftMax layer (all 🤗 Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f6eb6fe-b029-4ba5-b98c-11562155db8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[4.0195223e-02 9.5980471e-01]\n",
      " [9.9945587e-01 5.4418470e-04]], shape=(2, 2), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
    "print(predictions)\n",
    "\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603c1f69-7f2d-4263-a7be-35cafb730748",
   "metadata": {},
   "source": [
    "## 3.3 detail in model\n",
    "\n",
    "The AutoModel class and all of its relatives are actually simple wrappers over the wide variety of models available in the library. It’s a clever wrapper as it can automatically guess the appropriate model architecture for your checkpoint, and then instantiates a model with this architecture.\n",
    "\n",
    "However, if you know the type of model you want to use, you can use the class that defines its architecture directly. Let’s take a look at how this works with a BERT model.\n",
    "\n",
    "### 3.3.1 create a bert transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2a93260-ad07-4f1f-beea-81e5b2f72e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, TFBertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = TFBertModel(config)\n",
    "\n",
    "# While you haven’t seen what all of these attributes do yet, you should recognize some of them: \n",
    "# - the hidden_size attribute defines the size of the hidden_states vector, \n",
    "# - and num_hidden_layers defines the number of layers the Transformer model has.\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d83f3c-8063-4b62-8b61-00ea9125c7e5",
   "metadata": {},
   "source": [
    "#### 3.3.1.1 load from the default configuration\n",
    "\n",
    "The model can be used in this state, but it will output gibberish; it needs to be trained first. \n",
    "\n",
    "We could train the model from scratch on the task at hand, but as you saw in Chapter 1, this would require a long time and a lot of data, and it would have a non-negligible environmental impact. \n",
    "\n",
    "To avoid unnecessary and duplicated effort, it’s imperative to be able to share and reuse models that have already been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffce021-da93-4982-956e-33a41ac6deb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, TFBertModel\n",
    "\n",
    "config = BertConfig()\n",
    "model = TFBertModel(config)\n",
    "\n",
    "# Model is randomly initialized!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eaf2a4-3e59-4e69-9d35-599e471beb0e",
   "metadata": {},
   "source": [
    "#### 3.3.1.2 Loading already trained model\n",
    "\n",
    "Loading a Transformer model that is already trained is simple — we can do this using the from_pretrained() method.\n",
    "\n",
    "In the code sample below we didn’t use BertConfig, and instead loaded a pretrained model via the bert-base-cased identifier. This is a model checkpoint that was trained by the authors of BERT themselves; you can find more details about it in its [model card](https://huggingface.co/bert-base-cased).\n",
    "\n",
    "The identifier used to load the model can be the identifier of any model on the Model Hub, as long as it is compatible with the BERT architecture. The entire list of available BERT checkpoints can be found [here](https://huggingface.co/models?filter=bert)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b97110d-45ce-4ff7-8882-7fe9420ee8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23237ed6-a615-4325-aa33-c2d07a7d7bb3",
   "metadata": {},
   "source": [
    "#### 3.3.1.3 save your model checkpoints\n",
    "\n",
    "[here](https://huggingface.co/learn/nlp-course/zh-CN/chapter2/3?fw=tf#%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B) is the doc "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546d0696-0321-46a7-a93a-31e41e28576b",
   "metadata": {},
   "source": [
    "### 3.3.2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ed770ac-0f97-476b-86d6-c9f83e18fec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
      "array([[ 101, 7592,  999,  102],\n",
      "       [ 101, 4658, 1012,  102],\n",
      "       [ 101, 3835,  999,  102]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1],\n",
      "       [1, 1, 1, 1],\n",
      "       [1, 1, 1, 1]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]\n",
    "\n",
    "encoded_sequences = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "print(encoded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70b41e7b-e04c-41ad-97de-287026e4ccd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor: shape=(3, 4, 768), dtype=float32, numpy=\n",
      "array([[[ 0.20947652,  0.59224457, -1.0587867 , ..., -0.6082401 ,\n",
      "         -2.421259  ,  0.51466435],\n",
      "        [ 1.8963727 ,  0.59870255, -1.8604324 , ...,  0.4906728 ,\n",
      "         -1.5082163 ,  0.68668133],\n",
      "        [-0.39519897,  1.2889798 , -1.4721617 , ...,  0.33381462,\n",
      "         -0.5313246 ,  2.2940066 ],\n",
      "        [ 0.02544761,  0.08925232,  0.08103876, ..., -0.09978881,\n",
      "         -1.3950051 ,  0.47364238]],\n",
      "\n",
      "       [[ 0.26413783,  0.7517032 , -1.1615443 , ..., -0.10973767,\n",
      "         -2.062973  ,  0.8564927 ],\n",
      "        [ 0.9136969 ,  0.18195146, -1.7460833 , ...,  0.8136738 ,\n",
      "         -0.49246275,  1.1784    ],\n",
      "        [-1.3480021 , -0.7357795 , -1.226221  , ...,  0.63696945,\n",
      "         -0.46924728,  1.5807151 ],\n",
      "        [-0.04882637,  0.33790863,  0.00252043, ...,  0.20194522,\n",
      "         -1.0780818 ,  0.8518338 ]],\n",
      "\n",
      "       [[ 0.01836708,  0.64367527, -0.7602318 , ..., -0.4939745 ,\n",
      "         -2.0095007 ,  0.69358164],\n",
      "        [ 0.80830973,  0.9223389 , -1.4010136 , ...,  0.11480556,\n",
      "         -0.5419662 ,  0.6242263 ],\n",
      "        [-0.5837767 ,  1.3656062 , -1.1458869 , ...,  0.34254482,\n",
      "         -0.08925059,  2.4449365 ],\n",
      "        [-0.08138295,  0.24179973,  0.34502915, ..., -0.170169  ,\n",
      "         -0.95605874,  0.65882343]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(3, 768), dtype=float32, numpy=\n",
      "array([[-0.13353646, -0.67436296, -0.18856473, ..., -0.04980682,\n",
      "         0.41846687,  0.44057164],\n",
      "       [-0.05387908, -0.592937  , -0.21642329, ..., -0.01866785,\n",
      "         0.40613085,  0.4750938 ],\n",
      "       [-0.12935379, -0.6776982 , -0.02194682, ...,  0.01298611,\n",
      "         0.41389406,  0.3906389 ]], dtype=float32)>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "output = model(encoded_sequences)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7fddb9-a0ec-4033-b956-21ce890ea17a",
   "metadata": {},
   "source": [
    "## 3.4 detail in tokenizer\n",
    "\n",
    "### 3.4.1 Encoding\n",
    "\n",
    "Tokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. \n",
    "\n",
    "Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. \n",
    "\n",
    "Let’s take a look at some examples of tokenization algorithms:\n",
    "\n",
    "- Word-based\n",
    "- Character-based\n",
    "- Subword tokenization\n",
    "  - some more techniques:\n",
    "    - Byte-level BPE, as used in GPT-2\n",
    "    - WordPiece, as used in BERT\n",
    "    - SentencePiece or Unigram, as used in several multilingual models\n",
    " \n",
    "tokenization algorithms, can let raw text inputs to tokens:\n",
    "- raw inputs: \"Using a Transformer network is simple\"\n",
    "- tokens: ['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']\n",
    "\n",
    "then we need transfer tokens to input ids.\n",
    "\n",
    "```python\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "# [7993, 170, 11303, 1200, 2443, 1110, 3014]\n",
    "```\n",
    "\n",
    "### 3.4.2 Decoding\n",
    "\n",
    "```python\n",
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decoded_string)\n",
    "# 'Using a Transformer network is simple'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84434eab-72cf-4be5-8291-f662c7d742a2",
   "metadata": {},
   "source": [
    "## 3.5 detail in sequence\n",
    "\n",
    "First, let's organize what we learn before\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Note: [ids], not ids, need array.\n",
    "input_ids = tf.constant([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)\n",
    "```\n",
    "\n",
    "### 3.5.1 mulitple sequence\n",
    "\n",
    "```python\n",
    "# ... as before\n",
    "batched_ids = [ids, ids]\n",
    "\n",
    "input_ids = tf.constant(batched_ids)\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)\n",
    "```\n",
    "\n",
    "### 3.5.2 difference length sequences?\n",
    "\n",
    "```python\n",
    "# origin is this\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200]\n",
    "]\n",
    "```\n",
    "\n",
    "how to handle this?\n",
    "\n",
    "Answer:\n",
    "- **Padding the inputs**: we’ll use padding to make our tensors have a rectangular shape. Padding makes sure all our sentences have the same length by adding a special word called the padding token to the sentences with fewer values. \n",
    "- **Attention masks**: Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by the attention layers of the model).\n",
    "\n",
    "```python\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "print(model(tf.constant(sequence1_ids)).logits)\n",
    "print(model(tf.constant(sequence2_ids)).logits)\n",
    "print(model(tf.constant(batched_ids)).logits)  # will different with sequence2_ids\n",
    "print(model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask)).logits)  # will same with sequence2_ids\n",
    "```\n",
    "\n",
    "### 3.5.3 too long sequence\n",
    "\n",
    "With Transformer models, there is a limit to the lengths of the sequences we can pass the models. Most models handle sequences of up to 512 or 1024 tokens, and will crash when asked to process longer sequences. There are two solutions to this problem:\n",
    "\n",
    "- Use a model with a longer supported sequence length.\n",
    "- Truncate your sequences.\n",
    "\n",
    "Models have different supported sequence lengths, and some specialize in handling very long sequences. [Longformer](https://huggingface.co/docs/transformers/model_doc/longformer) is one example, and another is [LED](https://huggingface.co/docs/transformers/model_doc/led). If you’re working on a task that requires very long sequences, we recommend you take a look at those models.\n",
    "\n",
    "Otherwise, we recommend you truncate your sequences by specifying the max_sequence_length parameter:\n",
    "\n",
    "```python\n",
    "sequence = sequence[:max_sequence_length]\n",
    "```\n",
    "\n",
    "### 3.5.4\n",
    "\n",
    "token_type_ids: this is what tells the model which part of the input is the first sentence and which is the second sentence.\n",
    "\n",
    "```python\n",
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "inputs\n",
    "\"\"\"\n",
    "{ \n",
    "  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],\n",
    "  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
    "  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "}\n",
    "\"\"\"\n",
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])\n",
    "\"\"\"\n",
    "['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "> Note:\n",
    "> - if you select a different checkpoint, you won’t necessarily have the token_type_ids in your tokenized inputs (for instance, they’re not returned if you use a DistilBERT model).\n",
    "> - They are only returned when the model will know what to do with them, because it has seen them during its pretraining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151a2b38-1e28-40cd-9d85-cfcf47401bdb",
   "metadata": {},
   "source": [
    "## 3.6 Putting it all together\n",
    "\n",
    "In the last few sections, we’ve been trying our best to do most of the work by hand. We’ve explored how tokenizers work and looked at tokenization, conversion to input IDs, padding, truncation, and attention masks.\n",
    "\n",
    "However, as we saw in section 2, the 🤗 Transformers API can handle all of this for us with a high-level function that we’ll dive into here. When you call your tokenizer directly on the sentence, you get back inputs that are ready to pass through your model.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b7a0db-eaf5-4301-afcd-2a68fd92979e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
