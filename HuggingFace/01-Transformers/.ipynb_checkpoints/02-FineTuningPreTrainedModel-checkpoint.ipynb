{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7eabab2-8eca-44a4-a590-e07d69eb4b08",
   "metadata": {},
   "source": [
    "# 1 intuition about fine-tuning a pretrained model\n",
    "\n",
    "## 1.1 demo for train a sequence classifier\n",
    "\n",
    "Continuing with the example from the previous chapter, here is how we would train a sequence classifier on one batch in TensorFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28209ae-f775-462d-96b5-f4d4cb45d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "# Same as before\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\"))\n",
    "\n",
    "# This is new\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
    "labels = tf.convert_to_tensor([1, 1])\n",
    "model.train_on_batch(batch, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d133cc-7d0a-47ca-b4c4-d8ab7fb2bf70",
   "metadata": {},
   "source": [
    "# 2 use dataset from huggingface dataset hub\n",
    "\n",
    "## 2.1 load dataset\n",
    "\n",
    "Of course, just training the model on two sentences is not going to yield very good results. To get better results, you will need to prepare a bigger dataset.\n",
    "\n",
    "In this section we will use as an example the MRPC (Microsoft Research Paraphrase Corpus) dataset, introduced in a paper by William B. Dolan and Chris Brockett. The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing). We‚Äôve selected it for this chapter because it‚Äôs a small dataset, so it‚Äôs easy to experiment with training on it.\n",
    "\n",
    "The Hub doesn‚Äôt just contain models; it also has multiple datasets in lots of different languages. You can browse the datasets [here](https://huggingface.co/datasets).\n",
    "\n",
    "The ü§ó Datasets library provides a very simple command to download and cache a dataset on the Hub. We can download the MRPC dataset like this:\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets\n",
    "\"\"\"\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "        num_rows: 3668\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "        num_rows: 408\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "        num_rows: 1725\n",
    "    })\n",
    "})\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "This command downloads and caches the dataset, by default in ~/.cache/huggingface/datasets. Recall from Chapter 2 that you can customize your cache folder by setting the HF_HOME environment variable.\n",
    "\n",
    "Here is how to get the `train` dataset:\n",
    "```python\n",
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]\n",
    "\n",
    "raw_train_dataset.features\n",
    "\"\"\"\n",
    "{'sentence1': Value(dtype='string', id=None),\n",
    " 'sentence2': Value(dtype='string', id=None),\n",
    " 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),\n",
    " 'idx': Value(dtype='int32', id=None)}\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27c1cdc-6402-4aa1-baeb-fc5d577f8b18",
   "metadata": {},
   "source": [
    "## 2.2 preprocessing the dataset\n",
    "\n",
    "### 2.2.1 preprocess before\n",
    "\n",
    "To preprocess the dataset, we need to convert the text to numbers the model can make sense of. As you saw in the previous chapter, this is done with a tokenizer. \n",
    "\n",
    "We can feed the tokenizer one sentence or a list of sentences.\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])\n",
    "\n",
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")\n",
    "```\n",
    "\n",
    "### 2.2.2 preprocess batch\n",
    "\n",
    "```python\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "# Note that we‚Äôve left the padding argument out in our tokenization function for now. \n",
    "# This is because padding all the samples to the maximum length is not efficient: \n",
    "# - it‚Äôs better to pad the samples when we‚Äôre building a batch, as then we only need to pad to the maximum length in that batch, \n",
    "# - and not the maximum length in the entire dataset. \n",
    "# This can save a lot of time and processing power when the inputs have very variable lengths!\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets\n",
    "```\n",
    "\n",
    "You can even use multiprocessing when applying your preprocessing function with map() by passing along a `num_proc` argument. We didn‚Äôt do this here because the ü§ó Tokenizers library already uses multiple threads to tokenize our samples faster, but if you are not using a fast tokenizer backed by this library, this could speed up your preprocessing.\n",
    "\n",
    "### 2.2.3 dynamic padding\n",
    "\n",
    "The last thing we will need to do is pad all the examples to the length of the longest element when we batch elements together ‚Äî a technique we refer to as dynamic padding.\n",
    "\n",
    "```python\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "\n",
    "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "tf_validation_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "```\n",
    "\n",
    "## 2.3 Fine-tuning a model with Keras\n",
    "\n",
    "### 2.3.1 before code all in one\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "import numpy as np\n",
    "\n",
    "# load dataset\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# preprocess: dataset tokenization\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "# preprocess: batch\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "# preprocess: dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "\n",
    "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "tf_validation_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "```\n",
    "\n",
    "### 2.3.2 fine-tuning intuition\n",
    "\n",
    "```python\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    ")\n",
    "```\n",
    "\n",
    "### 2.3.3 optimize fine-tuning\n",
    "\n",
    "#### 2.3.3.1 learning rate\n",
    "\n",
    "From long experience, though, we know that transformer models benefit from a much lower learning rate than the default for Adam, which is 1e-3, also written as 10 to the power of -3, or 0.001. 5e-5 (0.00005), which is some twenty times lower, is a much better starting point.\n",
    "\n",
    "#### 2.3.3.2 decaying learning rate\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 3\n",
    "# ËÆ≠ÁªÉÊ≠•Êï∞ÊòØÊï∞ÊçÆÈõÜ‰∏≠ÁöÑÊ†∑Êú¨Êï∞Èô§‰ª•batch sizeÂÜç‰πò‰ª• epoch„ÄÇ\n",
    "# Ê≥®ÊÑèËøôÈáåÁöÑtf_train_datasetÊòØ‰∏Ä‰∏™ËΩ¨Âåñ‰∏∫batchÂêéÁöÑ tf.data.DatasetÔºå\n",
    "# ‰∏çÊòØÂéüÊù•ÁöÑ Hugging Face DatasetÔºåÊâÄ‰ª•ÂÆÉÁöÑ len() Â∑≤ÁªèÊòØ num_samples // batch_size„ÄÇ\n",
    "num_train_steps = len(tf_train_dataset) * num_epochs\n",
    "lr_scheduler = PolynomialDecay(\n",
    "    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "opt = Adam(learning_rate=lr_scheduler)\n",
    "```\n",
    "\n",
    "#### 2.3.3.3 use optimization\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n",
    "model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)\n",
    "```\n",
    "\n",
    "## 2.4 prediction\n",
    "\n",
    "```python\n",
    "preds = model.predict(tf_validation_dataset)[\"logits\"]\n",
    "class_preds = np.argmax(preds, axis=1)\n",
    "print(preds.shape, class_preds.shape)\n",
    "```\n",
    "\n",
    "## 2.5 evaluate\n",
    "\n",
    "```python\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=class_preds, references=raw_datasets[\"validation\"][\"label\"])\n",
    "```\n",
    "\n",
    "# 3 summary\n",
    "\n",
    "## 3.1 prepare dataset\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "## 1.1 load dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "## 1.2 preprocess dataset: batch process, dynamic padding\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "```\n",
    "\n",
    "## 3.2 train model\n",
    "\n",
    "### 3.2.1 trainer auto process dataset\n",
    "\n",
    "- Remove the columns corresponding to values the model does not expect (like the sentence1 and sentence2 columns).\n",
    "- Rename the column label to labels (because the model expects the argument to be named labels).\n",
    "- Set the format of the datasets so they return PyTorch tensors instead of lists.\n",
    "\n",
    "```python\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"tf\")\n",
    "tokenized_datasets[\"train\"].column_names\n",
    "# [\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"]\n",
    "```\n",
    "\n",
    "### 3.2.2 define dataloader\n",
    "\n",
    "```python\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# To quickly check there is no mistake in the data processing, we can inspect a batch like this.\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}\n",
    "\"\"\"\n",
    "{'attention_mask': torch.Size([8, 65]),\n",
    " 'input_ids': torch.Size([8, 65]),\n",
    " 'labels': torch.Size([8]),\n",
    " 'token_type_ids': torch.Size([8, 65])}\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### 3.2.3 define model\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "# To make sure that everything will go smoothly during training, we pass our batch to this model.\n",
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)\n",
    "# tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])\n",
    "```\n",
    "\n",
    "### 3.2.3 define optimizer\n",
    "\n",
    "```python\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "```\n",
    "\n",
    "### 3.2.4 define learning rate scheduler\n",
    "\n",
    "- epoch: the train numbers of all data.\n",
    "- training batches: Êï∞ÊçÆÈõÜ‰∏≠ÁöÑÊ†∑Êú¨Êï∞Èô§‰ª•batch size.  (which is the length of our training dataloader).\n",
    "\n",
    "```python\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps) \n",
    "# 1377\n",
    "```\n",
    "\n",
    "### 3.2.5 use GPU\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device\n",
    "# device(type='cuda')\n",
    "```\n",
    "\n",
    "### 3.2.6 use multiple GPU\n",
    "\n",
    "[doc](https://huggingface.co/learn/nlp-course/en/chapter3/4?fw=tf#supercharge-your-training-loop-with-accelerate)\n",
    "\n",
    "### 3.2.7 start train model\n",
    "\n",
    "To get some sense of when training will be finished, we add a progress bar over our number of training steps, using the tqdm library\n",
    "\n",
    "```python\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "```\n",
    "\n",
    "## 3.3 evaluate model\n",
    "\n",
    "```python\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()\n",
    "# {'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
