{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39f10fa6-7d2f-4ac0-936d-3f77d2d8508a",
   "metadata": {},
   "source": [
    "# 1 Transformer in Audio\n",
    "\n",
    "For audio tasks, the input and/or output sequences may be audio instead of text:\n",
    "\n",
    "- **Automatic speech recognition (ASR)**: The input is speech, the output is text.\n",
    "- **Speech synthesis (TTS)**: The input is text, the output is speech.\n",
    "- **Audio classification**: The input is audio, the output is a class probability — one for each element in the sequence or a single class probability for the entire sequence.\n",
    "- **Voice conversion or speech enhancement**: Both the input and output are audio.\n",
    "\n",
    "There are a few different ways to handle audio so it can be used with a transformer. The main consideration is whether to use the audio in its raw form — as a waveform — or to process it as a spectrogram instead.\n",
    "\n",
    "![transformers_in_audio](../images/03-transformers_in_audio.png)\n",
    "\n",
    "## 1.1 transformer input\n",
    "\n",
    "- text input:\n",
    "  - The input text is first tokenized, giving a sequence of text tokens.\n",
    "  - This sequence is sent through an input embedding layer to convert the tokens into 512-dimensional vectors.\n",
    "  - Those embedding vectors are then passed into the transformer encoder.\n",
    "\n",
    "- waveform input:\n",
    "  - After normalizing, the sequence of audio samples is turned into an embedding using a small convolutional neural network, known as the feature encoder.\n",
    " \n",
    "- Spectrogram input\n",
    " \n",
    "In both cases, waveform as well as spectrogram input, there is a small network in front of the transformer that converts the input into embeddings and then the transformer takes over to do its thing.\n",
    "\n",
    "## 1.2 transformer output\n",
    "\n",
    "- text output\n",
    "- Spectrogram output\n",
    "- waveform output\n",
    "\n",
    "# 2 difference structure for transformer\n",
    "\n",
    "## 2.1 CTC model: alignment of audio inputs the text outputs\n",
    "\n",
    "CTC or Connectionist Temporal Classification is a technique that is used with **encoder-only** transformer models for automatic speech recognition. Examples of such models are Wav2Vec2, HuBERT and M-CTC-T.\n",
    "\n",
    "Here’s the rub: In speech, we don’t know the **alignment** of the audio inputs and text outputs. We know that the order the speech is spoken in is the same as the order that the text is transcribed in (the alignment is so-called monotonic), but we don’t know how the characters in the transcription line up to the audio. This is where the CTC algorithm comes in.\n",
    "\n",
    "## 2.2 seq2seq model\n",
    "\n",
    "The CTC models discussed in the previous section used only the encoder part of the transformer architecture. When we also add the decoder to create an encoder-decoder model, this is referred to as a sequence-to-sequence model or seq2seq for short. The model maps a sequence of one kind of data to a sequence of another kind of data.\n",
    "\n",
    "With encoder-only transformer models, the encoder made a prediction for each element in the input sequence. Therefore, both input and output sequences will always have the same length. In the case of CTC models such as Wav2Vec2 the input waveform was first downsampled, but there still was one prediction for every 20 ms of audio.\n",
    "\n",
    "With a seq2seq model, there is no such one-to-one correspondence and the input and output sequences can have different lengths. That makes seq2seq models suitable for NLP tasks such as text summarization or translation between different languages — but also for audio tasks such as speech recognition.\n",
    "\n",
    "## 2.3 audio classification\n",
    "\n",
    "The goal of audio classification is to predict a class label for an audio input. The model can predict a single class label that covers the entire input sequence, or it can predict a label for every audio frame — typically every 20 milliseconds of input audio — in which case the model’s output is a sequence of class label probabilities. An example of the former is detecting what bird is making a particular sound; an example of the latter is speaker diarization, where the model predicts which speaker is speaking at any given moment.\n",
    "\n",
    "# 3 ASR\n",
    "\n",
    "## 3.1 Pre-trained models and datasets for audio classification\n",
    "\n",
    "### 3.1.1 CTC model\n",
    "\n",
    "Prior to 2022, CTC was the more popular of the two architectures, with encoder-only models such as Wav2Vec2, HuBERT and XLSR achieving breakthoughs in the pre-training / fine-tuning paradigm for speech. \n",
    "\n",
    "Comparing the target text to the predicted transcription, we can see that all words sound correct, but some are not spelled accurately. For example:\n",
    "\n",
    "- CHRISTMAUS vs. CHRISTMAS\n",
    "- ROSE vs. ROAST\n",
    "- SIMALYIS vs. SIMILES\n",
    "\n",
    "This highlights the shortcoming of a CTC model. A CTC model is essentially an `acoustic-only` model: it consists of an encoder which forms hidden-state representations from the audio inputs, and a linear layer which maps the hidden-states to characters.\n",
    "\n",
    "This means that the system almost entirely bases its prediction on the acoustic input it was given (the phonetic sounds of the audio), and so has a tendency to transcribe the audio in a phonetic way (e.g. CHRISTMAUS). \n",
    "\n",
    "It gives less importance to the language modelling context of previous and successive letters, and so is prone to phonetic spelling errors. \n",
    "\n",
    "### 3.1.2 Seq2Seq model\n",
    "\n",
    "In particular, the need for large amounts of training data has been a bottleneck in the advancement of Seq2Seq architectures for speech. Labelled speech data is difficult to come by, with the largest annotated datasets at the time clocking in at just 10,000 hours. This all changed in 2022 upon the release of Whisper.\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\n",
    ")\n",
    "dataset\n",
    "\"\"\"\n",
    "Dataset({\n",
    "    features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
    "    num_rows: 73\n",
    "})\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\", model=\"openai/whisper-base\", device=device\n",
    ")\n",
    "\n",
    "sample = dataset[2]\n",
    "\n",
    "pipe(sample[\"audio\"], max_new_tokens=256)\n",
    "```\n",
    "\n",
    "## 3.2 Long-Form Transcription and Timestamps\n",
    "\n",
    "### 3.2.1 concate a long audio\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "target_length_in_m = 5\n",
    "\n",
    "# 将分钟数转为秒数（* 60），再转为采用点数（* sampling rate）\n",
    "sampling_rate = pipe.feature_extractor.sampling_rate\n",
    "target_length_in_samples = target_length_in_m * 60 * sampling_rate\n",
    "\n",
    "# 遍历数据集，拼接样本直到长度达到目标\n",
    "long_audio = []\n",
    "for sample in dataset:\n",
    "    long_audio.extend(sample[\"audio\"][\"array\"])\n",
    "    if len(long_audio) > target_length_in_samples:\n",
    "        break\n",
    "\n",
    "long_audio = np.asarray(long_audio)\n",
    "\n",
    "# 结果如何？\n",
    "seconds = len(long_audio) / 16000\n",
    "minutes, seconds = divmod(seconds, 60)\n",
    "print(f\"Length of audio sample is {minutes} minutes {seconds:.2f} seconds\")\n",
    "# Length of audio sample is 5.0 minutes 17.22 seconds\n",
    "```\n",
    "\n",
    "```python\n",
    "pipe(\n",
    "    long_audio,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"task\": \"transcribe\"},\n",
    "    chunk_length_s=30,\n",
    "    batch_size=8,\n",
    ")\n",
    "```\n",
    "\n",
    "### 3.2.2 add timestamp\n",
    "\n",
    "```python\n",
    "pipe(\n",
    "    long_audio,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"task\": \"transcribe\"},\n",
    "    chunk_length_s=30,\n",
    "    batch_size=8,\n",
    "    return_timestamps=True,\n",
    ")[\"chunks\"]\n",
    "```\n",
    "\n",
    "### 3.2.3 about low-resource languages\n",
    "\n",
    "While the Whisper model performs extremely well on many high-resource languages, it has lower transcription and translation accuracy on low-resource languages, i.e. those with less readily available training data. There is also varying performance across different accents and dialects of certain languages, including lower accuracy for speakers of different genders, races, ages or other demographic criteria (c.f. Whisper paper).\n",
    "\n",
    "To boost the performance on low-resource languages, accents or dialects, we can take the pre-trained Whisper model and train it on a small corpus of appropriately selected data, in a process called fine-tuning. We’ll show that with as little as ten hours of additional data, we can improve the performance of the Whisper model by over 100% on a low-resource language. In the next section, we’ll cover the process behind selecting a dataset for fine-tuning.\n",
    "\n",
    "## 3.3 choose dataset\n",
    "\n",
    "Features of speech datasets\n",
    "1. Number of hours\n",
    "2. Domain\n",
    "3. Speaking style\n",
    "4. Transcription style\n",
    "\n",
    "To know if our fine-tuned model is any good, we’ll need a rigorous way of evaluating it on unseen data and measuring its transcription accuracy. We’ll cover exactly this in the next section!\n",
    "\n",
    "## 3.4 Evaluation metrics for ASR\n",
    "\n",
    "If you’re familiar with the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) from NLP, the metrics for assessing speech recognition systems will be familiar!\n",
    "\n",
    "When assessing speech recognition systems, we compare the system’s predictions to the target text transcriptions, annotating any errors that are present. We categorise these errors into one of three categories:\n",
    "\n",
    "- Substitutions (S): where we transcribe the wrong word in our prediction (“sit” instead of “sat”)\n",
    "- Insertions (I): where we add an extra word in our prediction\n",
    "- Deletions (D): where we remove a word in our prediction\n",
    "\n",
    "These error categories are the same for all speech recognition metrics. What differs is the level at which we compute these errors: we can either compute them on the word level or on the character level.\n",
    "\n",
    "### 3.4.1 word level: WER\n",
    "\n",
    "The word error rate (WER) metric is the ‘de facto’ metric for speech recognition.\n",
    "\n",
    "```python\n",
    "# pip install --upgrade evaluate jiwer\n",
    "\n",
    "from evaluate import load\n",
    "\n",
    "wer_metric = load(\"wer\")\n",
    "\n",
    "wer = wer_metric.compute(references=[reference], predictions=[prediction])\n",
    "\n",
    "print(wer)\n",
    "# 0.3333333333333333\n",
    "```\n",
    "\n",
    "Since the WER is the ratio of errors to number of words (N), there is no upper limit on the WER!\n",
    "\n",
    "### 3.4.2 word level: WAcc\n",
    "\n",
    "Word Accuracy: We can flip the WER around to give us a metric where higher is better. Rather than measuring the word error rate, we can measure the word accuracy (WAcc) of our system\n",
    "\n",
    "### 3.4.3 character level: CER\n",
    "\n",
    "Character Error Rate:  The character error rate (CER) assesses systems on the character level. This means we divide up our words into their individual characters, and annotate errors on a character-by-character basis.\n",
    "\n",
    "### 3.4.5 Which metric should I use\n",
    "\n",
    "In general, the WER is used far more than the CER for assessing speech systems. This is because the WER requires systems to have greater understanding of the context of the predictions. \n",
    "\n",
    "But, Certain languages, such as Mandarin and Japanese, have no notion of ‘words’, and so the WER is meaningless. Here, we revert to using the CER.\n",
    "\n",
    "### 3.4.6 Normalization and orthographic\n",
    "\n",
    "the predicted transcriptions will be fully formatted with casing and punctuation, a style referred to as orthographic.\n",
    "\n",
    "There is a happy medium between normalising and not normalising: we can train our systems on orthographic transcriptions, and then normalise the predictions and targets before computing the WER.\n",
    "\n",
    "This way, we train our systems to predict fully formatted text, but also benefit from the WER improvements we get by normalising the transcriptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "861b90cd-bc19-4658-b224-b3acdc397d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he tells us that at this festive season of the year with christmas and roast beef looming before us similes drawn from eating and its results occur most readily to the mind'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "reference = \"HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAS AND ROAST BEEF LOOMING BEFORE US SIMILES DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND\"\n",
    "normalized_referece = normalizer(reference)\n",
    "normalized_referece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b966a2-ef27-45fd-9202-c1ee97b04f62",
   "metadata": {},
   "source": [
    "## 3.5 evaluate whisper-samll in 'dv'\n",
    "\n",
    "```python\n",
    "# 1 prepare pipeline\n",
    "# ====================\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    torch_dtype = torch.float16\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    torch_dtype = torch.float32\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-small\",\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# 2 prepare dataset\n",
    "# ====================\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "common_voice_test = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"test\"\n",
    ")\n",
    "\n",
    "# 3 directly predict by origin model\n",
    "# ====================\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "# 运行流式推理\n",
    "for prediction in tqdm(\n",
    "    pipe(\n",
    "        KeyDataset(common_voice_test, \"audio\"),\n",
    "        max_new_tokens=128,\n",
    "        generate_kwargs={\"task\": \"transcribe\"},\n",
    "        batch_size=32,\n",
    "    ),\n",
    "    total=len(common_voice_test),\n",
    "):\n",
    "    all_predictions.append(prediction[\"text\"])\n",
    "\n",
    "# 4 evaluate metrics: WER with orthograpic\n",
    "# ====================\n",
    "\n",
    "from evaluate import load\n",
    "\n",
    "wer_metric = load(\"wer\")\n",
    "\n",
    "wer_ortho = 100 * wer_metric.compute(\n",
    "    references=common_voice_test[\"sentence\"], predictions=all_predictions\n",
    ")\n",
    "wer_ortho\n",
    "# 167.29577268612022\n",
    "\n",
    "# 4 evaluate metrics: WER without orthograpic\n",
    "# ====================\n",
    "\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "# 计算规范化 WER\n",
    "all_predictions_norm = [normalizer(pred) for pred in all_predictions]\n",
    "all_references_norm = [normalizer(label) for label in common_voice_test[\"sentence\"]]\n",
    "\n",
    "# 过滤掉参考文本被规范化后为零值的样本\n",
    "all_predictions_norm = [\n",
    "    all_predictions_norm[i]\n",
    "    for i in range(len(all_predictions_norm))\n",
    "    if len(all_references_norm[i]) > 0\n",
    "]\n",
    "all_references_norm = [\n",
    "    all_references_norm[i]\n",
    "    for i in range(len(all_references_norm))\n",
    "    if len(all_references_norm[i]) > 0\n",
    "]\n",
    "\n",
    "wer = 100 * wer_metric.compute(\n",
    "    references=all_references_norm, predictions=all_predictions_norm\n",
    ")\n",
    "\n",
    "wer\n",
    "# 125.69809089960707\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d967ee6-c9b7-4b79-acc1-177b88e9dc19",
   "metadata": {},
   "source": [
    "## 3.6 fine-tuning model\n",
    "\n",
    "### 3.6.1 prepare env\n",
    "\n",
    "```python\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "\"\"\"\n",
    "Login successful\n",
    "Your token has been saved to /root/.huggingface/token\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### 3.6.2 prepare dataset\n",
    "\n",
    "```python\n",
    "\n",
    "# load dataset\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"train+validation\"\n",
    ")\n",
    "common_voice[\"test\"] = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"test\"\n",
    ")\n",
    "\n",
    "print(common_voice)\n",
    "\n",
    "# process columns\n",
    "common_voice = common_voice.select_columns([\"audio\", \"sentence\"])\n",
    "```\n",
    "\n",
    "### 3.6.3 prepare pipeline: Model for Transfer Learning\n",
    "\n",
    "**ASR pipeline**\n",
    "\n",
    "The ASR pipeline can be de-composed into three stages:\n",
    "\n",
    "- The feature extractor which pre-processes the raw audio-inputs to log-mel spectrograms\n",
    "- The model which performs the sequence-to-sequence mapping\n",
    "- The tokenizer which post-processes the predicted tokens to text\n",
    "\n",
    "**fine-tune it on a new language**\n",
    "\n",
    "When you fine-tune it on a new language, Whisper does a good job at leveraging its knowledge of the other 96 languages it’s pre-trained on. Largely speaking, all modern languages will be linguistically similar to at least one of the 96 languages Whisper already knows, so we’ll fall under this paradigm of cross-lingual knowledge representation.\n",
    "\n",
    "What we need to do to fine-tune Whisper on a new language is find the language most similar that Whisper was pre-trained on. The Wikipedia article for Dhivehi states that Dhivehi is closely related to the Sinhalese language of Sri Lanka. If we check the language codes again, we can see that Sinhalese is present in the Whisper language set, so we can safely set our language argument to \"sinhalese\".\n",
    "\n",
    "```python\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    \"openai/whisper-small\", language=\"sinhalese\", task=\"transcribe\"\n",
    ")\n",
    "```\n",
    "\n",
    "### 3.6.4 preprocess dataset\n",
    "\n",
    "```python\n",
    "common_voice[\"train\"].features\n",
    "\"\"\"\n",
    "{'audio': Audio(sampling_rate=48000, mono=True, decode=True, id=None),\n",
    " 'sentence': Value(dtype='string', id=None)}\n",
    "\"\"\"\n",
    "\n",
    "# resample\n",
    "from datasets import Audio\n",
    "\n",
    "sampling_rate = processor.feature_extractor.sampling_rate\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\n",
    "\n",
    "# ignore too long examples\n",
    "max_input_length = 30.0\n",
    "\n",
    "def is_audio_in_length_range(length):\n",
    "    return length < max_input_length\n",
    "\n",
    "common_voice[\"train\"] = common_voice[\"train\"].filter(\n",
    "    is_audio_in_length_range,\n",
    "    input_columns=[\"input_length\"],\n",
    ")\n",
    "```\n",
    "\n",
    "### 3.6.5 tokenize audio(intput feature, log-mel)\n",
    "\n",
    "Now we can write a function to prepare our data ready for the model:\n",
    "- We load and resample the audio data on a sample-by-sample basis by calling sample[\"audio\"]. As explained above, 🤗 Datasets performs any necessary resampling operations on the fly.\n",
    "- We use the feature extractor to compute the log-mel spectrogram input features from our 1-dimensional audio array.\n",
    "- We encode the transcriptions to label ids through the use of the tokenizer.\n",
    "\n",
    "```python\n",
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "\n",
    "    example = processor(\n",
    "        audio=audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        text=example[\"sentence\"],\n",
    "    )\n",
    "\n",
    "    # 计算输入音频样本的长度，以秒计\n",
    "    example[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
    "\n",
    "    return example\n",
    "\n",
    "common_voice = common_voice.map(\n",
    "    prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1\n",
    ")\n",
    "```\n",
    "\n",
    "### 3.6.6 define a data collator\n",
    "\n",
    "Training and Evaluation\n",
    "Now that we’ve prepared our data, we’re ready to dive into the training pipeline. The 🤗 Trainer will do much of the heavy lifting for us. All we have to do is:\n",
    "\n",
    "Define a data collator: the data collator takes our pre-processed data and prepares PyTorch tensors ready for the model.\n",
    "\n",
    "Evaluation metrics: during evaluation, we want to evaluate the model using the word error rate (WER) metric. We need to define a compute_metrics function that handles this computation.\n",
    "\n",
    "Load a pre-trained checkpoint: we need to load a pre-trained checkpoint and configure it correctly for training.\n",
    "\n",
    "Define the training arguments: these will be used by the 🤗 Trainer in constructing the training schedule.\n",
    "\n",
    "Once we’ve fine-tuned the model, we will evaluate it on the test data to verify that we have correctly trained it to transcribe speech in Dhivehi.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(\n",
    "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [\n",
    "            {\"input_features\": feature[\"input_features\"][0]} for feature in features\n",
    "        ]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "```\n",
    "\n",
    "### 3.6.7 define Evaluation Metrics\n",
    "\n",
    "```python\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # 用 pad_token_id 替换 -100\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # 我们希望在计算指标时不要组合起词元\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # 计算普通的 WER\n",
    "    wer_ortho = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    # 计算标准化的 WER\n",
    "    pred_str_norm = [normalizer(pred) for pred in pred_str]\n",
    "    label_str_norm = [normalizer(label) for label in label_str]\n",
    "    # 过滤，从而在评估时只计算 reference 非空的样本\n",
    "    pred_str_norm = [\n",
    "        pred_str_norm[i] for i in range(len(pred_str_norm)) if len(label_str_norm[i]) > 0\n",
    "    ]\n",
    "    label_str_norm = [\n",
    "        label_str_norm[i]\n",
    "        for i in range(len(label_str_norm))\n",
    "        if len(label_str_norm[i]) > 0\n",
    "    ]\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str_norm, references=label_str_norm)\n",
    "\n",
    "    return {\"wer_ortho\": wer_ortho, \"wer\": wer}\n",
    "```\n",
    "\n",
    "### 3.6.8 Load a pre-trained checkpoint\n",
    "\n",
    "```python\n",
    "\n",
    "# 1 load checkpoint\n",
    "\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "```\n",
    "\n",
    "### 3.6.9 define the training arguments\n",
    "\n",
    "For more detail on the training arguments, refer to the Seq2SeqTrainingArguments [docs](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments).\n",
    "\n",
    "```python\n",
    "# some setting\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# train args: disable cache during training since it's incompatible with gradient checkpointing\n",
    "model.config.use_cache = False\n",
    "\n",
    "# generate args: 为生成设置语言和任务，并重新启用缓存\n",
    "model.generate = partial(\n",
    "    model.generate, language=\"sinhalese\", task=\"transcribe\", use_cache=True\n",
    ")\n",
    "\n",
    "# train args\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-dv\",  # 在 HF Hub 上的输出目录的名字\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # 每次 batch size 下调到一半就把这个参数上调到两倍\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    warmup_steps=50,\n",
    "    max_steps=500,  # 如果您有自己的 GPU 或者 Colab 付费计划，上调到 4000\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "# We can forward the training arguments to the 🤗 Trainer along with our model, dataset, data collator and compute_metrics function:\n",
    "\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")\n",
    "```\n",
    "\n",
    "### 3.6.10 train\n",
    "\n",
    "```python\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "### 3.6.11 share model\n",
    "\n",
    "```python\n",
    "kwargs = {\n",
    "    \"dataset_tags\": \"mozilla-foundation/common_voice_13_0\",\n",
    "    \"dataset\": \"Common Voice 13\",  # 训练数据集\n",
    "    \"language\": \"dv\",\n",
    "    \"model_name\": \"Whisper Small Dv - Sanchit Gandhi\",  # 给模型起个“漂亮”的名字\n",
    "    \"finetuned_from\": \"openai/whisper-small\",\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "}\n",
    "\n",
    "trainer.push_to_hub(**kwargs)\n",
    "```\n",
    "\n",
    "## 3.7 build app demo\n",
    "\n",
    "- [gradio app demo](https://huggingface.co/learn/audio-course/en/chapter5/demo)\n",
    "\n",
    "# 4 more doc\n",
    "\n",
    "* [Whisper Talk](https://www.youtube.com/live/fZMiD8sDzzg?feature=share) by Jong Wook Kim: a presentation on the Whisper model, explaining the motivation, architecture, training and results, delivered by Whisper author Jong Wook Kim\n",
    "* [End-to-End Speech Benchmark (ESB)](https://arxiv.org/abs/2210.13352): a paper that comprehensively argues for using the orthographic WER as opposed to the normalised WER for evaluating ASR systems and presents an accompanying benchmark\n",
    "* [Fine-Tuning Whisper for Multilingual ASR](https://huggingface.co/blog/fine-tune-whisper): an in-depth blog post that explains how the Whisper model works in more detail, and the pre- and post-processing steps involved with the feature extractor and tokenizer\n",
    "* [Fine-tuning MMS Adapter Models for Multi-Lingual ASR](https://huggingface.co/blog/mms_adapters): an end-to-end guide for fine-tuning Meta AI's new [MMS](https://ai.facebook.com/blog/multilingual-model-speech-recognition/) speech recognition models, freezing the base model weights and only fine-tuning a small number of *adapter* layers\n",
    "* [Boosting Wav2Vec2 with n-grams in 🤗 Transformers](https://huggingface.co/blog/wav2vec2-with-ngram): a blog post for combining CTC models with external language models (LMs) to combat spelling and punctuation errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ec68e4-89a6-413f-8bde-ba14f06ea0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
