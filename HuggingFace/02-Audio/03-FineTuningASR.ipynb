{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39f10fa6-7d2f-4ac0-936d-3f77d2d8508a",
   "metadata": {},
   "source": [
    "# 1 Transformer in Audio\n",
    "\n",
    "For audio tasks, the input and/or output sequences may be audio instead of text:\n",
    "\n",
    "- **Automatic speech recognition (ASR)**: The input is speech, the output is text.\n",
    "- **Speech synthesis (TTS)**: The input is text, the output is speech.\n",
    "- **Audio classification**: The input is audio, the output is a class probability â€” one for each element in the sequence or a single class probability for the entire sequence.\n",
    "- **Voice conversion or speech enhancement**: Both the input and output are audio.\n",
    "\n",
    "There are a few different ways to handle audio so it can be used with a transformer. The main consideration is whether to use the audio in its raw form â€” as a waveform â€” or to process it as a spectrogram instead.\n",
    "\n",
    "![transformers_in_audio](../images/03-transformers_in_audio.png)\n",
    "\n",
    "## 1.1 transformer input\n",
    "\n",
    "- text input:\n",
    "  - The input text is first tokenized, giving a sequence of text tokens.\n",
    "  - This sequence is sent through an input embedding layer to convert the tokens into 512-dimensional vectors.\n",
    "  - Those embedding vectors are then passed into the transformer encoder.\n",
    "\n",
    "- waveform input:\n",
    "  - After normalizing, the sequence of audio samples is turned into an embedding using a small convolutional neural network, known as the feature encoder.\n",
    " \n",
    "- Spectrogram input\n",
    " \n",
    "In both cases, waveform as well as spectrogram input, there is a small network in front of the transformer that converts the input into embeddings and then the transformer takes over to do its thing.\n",
    "\n",
    "## 1.2 transformer output\n",
    "\n",
    "- text output\n",
    "- Spectrogram output\n",
    "- waveform output\n",
    "\n",
    "# 2 difference structure for transformer\n",
    "\n",
    "## 2.1 CTC model: alignment of audio inputs the text outputs\n",
    "\n",
    "CTC or Connectionist Temporal Classification is a technique that is used with **encoder-only** transformer models for automatic speech recognition. Examples of such models are Wav2Vec2, HuBERT and M-CTC-T.\n",
    "\n",
    "Hereâ€™s the rub: In speech, we donâ€™t know the **alignment** of the audio inputs and text outputs. We know that the order the speech is spoken in is the same as the order that the text is transcribed in (the alignment is so-called monotonic), but we donâ€™t know how the characters in the transcription line up to the audio. This is where the CTC algorithm comes in.\n",
    "\n",
    "## 2.2 seq2seq model\n",
    "\n",
    "The CTC models discussed in the previous section used only the encoder part of the transformer architecture. When we also add the decoder to create an encoder-decoder model, this is referred to as a sequence-to-sequence model or seq2seq for short. The model maps a sequence of one kind of data to a sequence of another kind of data.\n",
    "\n",
    "With encoder-only transformer models, the encoder made a prediction for each element in the input sequence. Therefore, both input and output sequences will always have the same length. In the case of CTC models such as Wav2Vec2 the input waveform was first downsampled, but there still was one prediction for every 20 ms of audio.\n",
    "\n",
    "With a seq2seq model, there is no such one-to-one correspondence and the input and output sequences can have different lengths. That makes seq2seq models suitable for NLP tasks such as text summarization or translation between different languages â€” but also for audio tasks such as speech recognition.\n",
    "\n",
    "## 2.3 audio classification\n",
    "\n",
    "The goal of audio classification is to predict a class label for an audio input. The model can predict a single class label that covers the entire input sequence, or it can predict a label for every audio frame â€” typically every 20 milliseconds of input audio â€” in which case the modelâ€™s output is a sequence of class label probabilities. An example of the former is detecting what bird is making a particular sound; an example of the latter is speaker diarization, where the model predicts which speaker is speaking at any given moment.\n",
    "\n",
    "# 3 ASR\n",
    "\n",
    "## 3.1 Pre-trained models and datasets for audio classification\n",
    "\n",
    "### 3.1.1 CTC model\n",
    "\n",
    "Prior to 2022, CTC was the more popular of the two architectures, with encoder-only models such as Wav2Vec2, HuBERT and XLSR achieving breakthoughs in the pre-training / fine-tuning paradigm for speech. \n",
    "\n",
    "Comparing the target text to the predicted transcription, we can see that all words sound correct, but some are not spelled accurately. For example:\n",
    "\n",
    "- CHRISTMAUS vs. CHRISTMAS\n",
    "- ROSE vs. ROAST\n",
    "- SIMALYIS vs. SIMILES\n",
    "\n",
    "This highlights the shortcoming of a CTC model. A CTC model is essentially an `acoustic-only` model: it consists of an encoder which forms hidden-state representations from the audio inputs, and a linear layer which maps the hidden-states to characters.\n",
    "\n",
    "This means that the system almost entirely bases its prediction on the acoustic input it was given (the phonetic sounds of the audio), and so has a tendency to transcribe the audio in a phonetic way (e.g. CHRISTMAUS). \n",
    "\n",
    "It gives less importance to the language modelling context of previous and successive letters, and so is prone to phonetic spelling errors. \n",
    "\n",
    "### 3.1.2 Seq2Seq model\n",
    "\n",
    "In particular, the need for large amounts of training data has been a bottleneck in the advancement of Seq2Seq architectures for speech. Labelled speech data is difficult to come by, with the largest annotated datasets at the time clocking in at just 10,000 hours. This all changed in 2022 upon the release of Whisper.\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\n",
    ")\n",
    "dataset\n",
    "\"\"\"\n",
    "Dataset({\n",
    "    features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
    "    num_rows: 73\n",
    "})\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\", model=\"openai/whisper-base\", device=device\n",
    ")\n",
    "\n",
    "sample = dataset[2]\n",
    "\n",
    "pipe(sample[\"audio\"], max_new_tokens=256)\n",
    "```\n",
    "\n",
    "## 3.2 Long-Form Transcription and Timestamps\n",
    "\n",
    "### 3.2.1 concate a long audio\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "target_length_in_m = 5\n",
    "\n",
    "# å°†åˆ†é’Ÿæ•°è½¬ä¸ºç§’æ•°ï¼ˆ* 60ï¼‰ï¼Œå†è½¬ä¸ºé‡‡ç”¨ç‚¹æ•°ï¼ˆ* sampling rateï¼‰\n",
    "sampling_rate = pipe.feature_extractor.sampling_rate\n",
    "target_length_in_samples = target_length_in_m * 60 * sampling_rate\n",
    "\n",
    "# éå†æ•°æ®é›†ï¼Œæ‹¼æ¥æ ·æœ¬ç›´åˆ°é•¿åº¦è¾¾åˆ°ç›®æ ‡\n",
    "long_audio = []\n",
    "for sample in dataset:\n",
    "    long_audio.extend(sample[\"audio\"][\"array\"])\n",
    "    if len(long_audio) > target_length_in_samples:\n",
    "        break\n",
    "\n",
    "long_audio = np.asarray(long_audio)\n",
    "\n",
    "# ç»“æœå¦‚ä½•ï¼Ÿ\n",
    "seconds = len(long_audio) / 16000\n",
    "minutes, seconds = divmod(seconds, 60)\n",
    "print(f\"Length of audio sample is {minutes} minutes {seconds:.2f} seconds\")\n",
    "# Length of audio sample is 5.0 minutes 17.22 seconds\n",
    "```\n",
    "\n",
    "```python\n",
    "pipe(\n",
    "    long_audio,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"task\": \"transcribe\"},\n",
    "    chunk_length_s=30,\n",
    "    batch_size=8,\n",
    ")\n",
    "```\n",
    "\n",
    "### 3.2.2 add timestamp\n",
    "\n",
    "```python\n",
    "pipe(\n",
    "    long_audio,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"task\": \"transcribe\"},\n",
    "    chunk_length_s=30,\n",
    "    batch_size=8,\n",
    "    return_timestamps=True,\n",
    ")[\"chunks\"]\n",
    "```\n",
    "\n",
    "### 3.2.3 about low-resource languages\n",
    "\n",
    "While the Whisper model performs extremely well on many high-resource languages, it has lower transcription and translation accuracy on low-resource languages, i.e. those with less readily available training data. There is also varying performance across different accents and dialects of certain languages, including lower accuracy for speakers of different genders, races, ages or other demographic criteria (c.f. Whisper paper).\n",
    "\n",
    "To boost the performance on low-resource languages, accents or dialects, we can take the pre-trained Whisper model and train it on a small corpus of appropriately selected data, in a process called fine-tuning. Weâ€™ll show that with as little as ten hours of additional data, we can improve the performance of the Whisper model by over 100% on a low-resource language. In the next section, weâ€™ll cover the process behind selecting a dataset for fine-tuning.\n",
    "\n",
    "## 3.3 choose dataset\n",
    "\n",
    "Features of speech datasets\n",
    "1. Number of hours\n",
    "2. Domain\n",
    "3. Speaking style\n",
    "4. Transcription style\n",
    "\n",
    "To know if our fine-tuned model is any good, weâ€™ll need a rigorous way of evaluating it on unseen data and measuring its transcription accuracy. Weâ€™ll cover exactly this in the next section!\n",
    "\n",
    "## 3.4 Evaluation metrics for ASR\n",
    "\n",
    "If youâ€™re familiar with the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) from NLP, the metrics for assessing speech recognition systems will be familiar!\n",
    "\n",
    "When assessing speech recognition systems, we compare the systemâ€™s predictions to the target text transcriptions, annotating any errors that are present. We categorise these errors into one of three categories:\n",
    "\n",
    "- Substitutions (S): where we transcribe the wrong word in our prediction (â€œsitâ€ instead of â€œsatâ€)\n",
    "- Insertions (I): where we add an extra word in our prediction\n",
    "- Deletions (D): where we remove a word in our prediction\n",
    "\n",
    "These error categories are the same for all speech recognition metrics. What differs is the level at which we compute these errors: we can either compute them on the word level or on the character level.\n",
    "\n",
    "### 3.4.1 word level: WER\n",
    "\n",
    "The word error rate (WER) metric is the â€˜de factoâ€™ metric for speech recognition.\n",
    "\n",
    "```python\n",
    "# pip install --upgrade evaluate jiwer\n",
    "\n",
    "from evaluate import load\n",
    "\n",
    "wer_metric = load(\"wer\")\n",
    "\n",
    "wer = wer_metric.compute(references=[reference], predictions=[prediction])\n",
    "\n",
    "print(wer)\n",
    "# 0.3333333333333333\n",
    "```\n",
    "\n",
    "Since the WER is the ratio of errors to number of words (N), there is no upper limit on the WER!\n",
    "\n",
    "### 3.4.2 word level: WAcc\n",
    "\n",
    "Word Accuracy: We can flip the WER around to give us a metric where higher is better. Rather than measuring the word error rate, we can measure the word accuracy (WAcc) of our system\n",
    "\n",
    "### 3.4.3 character level: CER\n",
    "\n",
    "Character Error Rate:  The character error rate (CER) assesses systems on the character level. This means we divide up our words into their individual characters, and annotate errors on a character-by-character basis.\n",
    "\n",
    "### 3.4.5 Which metric should I use\n",
    "\n",
    "In general, the WER is used far more than the CER for assessing speech systems. This is because the WER requires systems to have greater understanding of the context of the predictions. \n",
    "\n",
    "But, Certain languages, such as Mandarin and Japanese, have no notion of â€˜wordsâ€™, and so the WER is meaningless. Here, we revert to using the CER.\n",
    "\n",
    "### 3.4.6 Normalization and orthographic\n",
    "\n",
    "the predicted transcriptions will be fully formatted with casing and punctuation, a style referred to as orthographic.\n",
    "\n",
    "There is a happy medium between normalising and not normalising: we can train our systems on orthographic transcriptions, and then normalise the predictions and targets before computing the WER.\n",
    "\n",
    "This way, we train our systems to predict fully formatted text, but also benefit from the WER improvements we get by normalising the transcriptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "861b90cd-bc19-4658-b224-b3acdc397d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he tells us that at this festive season of the year with christmas and roast beef looming before us similes drawn from eating and its results occur most readily to the mind'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "reference = \"HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAS AND ROAST BEEF LOOMING BEFORE US SIMILES DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND\"\n",
    "normalized_referece = normalizer(reference)\n",
    "normalized_referece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b966a2-ef27-45fd-9202-c1ee97b04f62",
   "metadata": {},
   "source": [
    "## 3.5 evaluate whisper-samll in 'dv'\n",
    "\n",
    "```python\n",
    "# 1 prepare pipeline\n",
    "# ====================\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    torch_dtype = torch.float16\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    torch_dtype = torch.float32\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-small\",\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# 2 prepare dataset\n",
    "# ====================\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "common_voice_test = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"test\"\n",
    ")\n",
    "\n",
    "# 3 directly predict by origin model\n",
    "# ====================\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "# è¿è¡Œæµå¼æ¨ç†\n",
    "for prediction in tqdm(\n",
    "    pipe(\n",
    "        KeyDataset(common_voice_test, \"audio\"),\n",
    "        max_new_tokens=128,\n",
    "        generate_kwargs={\"task\": \"transcribe\"},\n",
    "        batch_size=32,\n",
    "    ),\n",
    "    total=len(common_voice_test),\n",
    "):\n",
    "    all_predictions.append(prediction[\"text\"])\n",
    "\n",
    "# 4 evaluate metrics: WER with orthograpic\n",
    "# ====================\n",
    "\n",
    "from evaluate import load\n",
    "\n",
    "wer_metric = load(\"wer\")\n",
    "\n",
    "wer_ortho = 100 * wer_metric.compute(\n",
    "    references=common_voice_test[\"sentence\"], predictions=all_predictions\n",
    ")\n",
    "wer_ortho\n",
    "# 167.29577268612022\n",
    "\n",
    "# 4 evaluate metrics: WER without orthograpic\n",
    "# ====================\n",
    "\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "# è®¡ç®—è§„èŒƒåŒ– WER\n",
    "all_predictions_norm = [normalizer(pred) for pred in all_predictions]\n",
    "all_references_norm = [normalizer(label) for label in common_voice_test[\"sentence\"]]\n",
    "\n",
    "# è¿‡æ»¤æ‰å‚è€ƒæ–‡æœ¬è¢«è§„èŒƒåŒ–åä¸ºé›¶å€¼çš„æ ·æœ¬\n",
    "all_predictions_norm = [\n",
    "    all_predictions_norm[i]\n",
    "    for i in range(len(all_predictions_norm))\n",
    "    if len(all_references_norm[i]) > 0\n",
    "]\n",
    "all_references_norm = [\n",
    "    all_references_norm[i]\n",
    "    for i in range(len(all_references_norm))\n",
    "    if len(all_references_norm[i]) > 0\n",
    "]\n",
    "\n",
    "wer = 100 * wer_metric.compute(\n",
    "    references=all_references_norm, predictions=all_predictions_norm\n",
    ")\n",
    "\n",
    "wer\n",
    "# 125.69809089960707\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d967ee6-c9b7-4b79-acc1-177b88e9dc19",
   "metadata": {},
   "source": [
    "## 3.6 fine-tuning model\n",
    "\n",
    "### 3.6.1 prepare env\n",
    "\n",
    "```python\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "\"\"\"\n",
    "Login successful\n",
    "Your token has been saved to /root/.huggingface/token\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### 3.6.2 prepare dataset\n",
    "\n",
    "```python\n",
    "\n",
    "# load dataset\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"train+validation\"\n",
    ")\n",
    "common_voice[\"test\"] = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"test\"\n",
    ")\n",
    "\n",
    "print(common_voice)\n",
    "\n",
    "# process columns\n",
    "common_voice = common_voice.select_columns([\"audio\", \"sentence\"])\n",
    "```\n",
    "\n",
    "### 3.6.3 prepare pipeline: Model for Transfer Learning\n",
    "\n",
    "**ASR pipeline**\n",
    "\n",
    "The ASR pipeline can be de-composed into three stages:\n",
    "\n",
    "- The feature extractor which pre-processes the raw audio-inputs to log-mel spectrograms\n",
    "- The model which performs the sequence-to-sequence mapping\n",
    "- The tokenizer which post-processes the predicted tokens to text\n",
    "\n",
    "**fine-tune it on a new language**\n",
    "\n",
    "When you fine-tune it on a new language, Whisper does a good job at leveraging its knowledge of the other 96 languages itâ€™s pre-trained on. Largely speaking, all modern languages will be linguistically similar to at least one of the 96 languages Whisper already knows, so weâ€™ll fall under this paradigm of cross-lingual knowledge representation.\n",
    "\n",
    "What we need to do to fine-tune Whisper on a new language is find the language most similar that Whisper was pre-trained on. The Wikipedia article for Dhivehi states that Dhivehi is closely related to the Sinhalese language of Sri Lanka. If we check the language codes again, we can see that Sinhalese is present in the Whisper language set, so we can safely set our language argument to \"sinhalese\".\n",
    "\n",
    "```python\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    \"openai/whisper-small\", language=\"sinhalese\", task=\"transcribe\"\n",
    ")\n",
    "```\n",
    "\n",
    "### 3.6.4 preprocess dataset\n",
    "\n",
    "```python\n",
    "common_voice[\"train\"].features\n",
    "\"\"\"\n",
    "{'audio': Audio(sampling_rate=48000, mono=True, decode=True, id=None),\n",
    " 'sentence': Value(dtype='string', id=None)}\n",
    "\"\"\"\n",
    "\n",
    "# resample\n",
    "from datasets import Audio\n",
    "\n",
    "sampling_rate = processor.feature_extractor.sampling_rate\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\n",
    "\n",
    "# ignore too long examples\n",
    "max_input_length = 30.0\n",
    "\n",
    "def is_audio_in_length_range(length):\n",
    "    return length < max_input_length\n",
    "\n",
    "common_voice[\"train\"] = common_voice[\"train\"].filter(\n",
    "    is_audio_in_length_range,\n",
    "    input_columns=[\"input_length\"],\n",
    ")\n",
    "```\n",
    "\n",
    "### 3.6.5 tokenize audio(intput feature, log-mel)\n",
    "\n",
    "Now we can write a function to prepare our data ready for the model:\n",
    "- We load and resample the audio data on a sample-by-sample basis by calling sample[\"audio\"]. As explained above, ğŸ¤— Datasets performs any necessary resampling operations on the fly.\n",
    "- We use the feature extractor to compute the log-mel spectrogram input features from our 1-dimensional audio array.\n",
    "- We encode the transcriptions to label ids through the use of the tokenizer.\n",
    "\n",
    "```python\n",
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "\n",
    "    example = processor(\n",
    "        audio=audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        text=example[\"sentence\"],\n",
    "    )\n",
    "\n",
    "    # è®¡ç®—è¾“å…¥éŸ³é¢‘æ ·æœ¬çš„é•¿åº¦ï¼Œä»¥ç§’è®¡\n",
    "    example[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
    "\n",
    "    return example\n",
    "\n",
    "common_voice = common_voice.map(\n",
    "    prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1\n",
    ")\n",
    "```\n",
    "\n",
    "### 3.6.6 define a data collator\n",
    "\n",
    "Training and Evaluation\n",
    "Now that weâ€™ve prepared our data, weâ€™re ready to dive into the training pipeline. The ğŸ¤— Trainer will do much of the heavy lifting for us. All we have to do is:\n",
    "\n",
    "Define a data collator: the data collator takes our pre-processed data and prepares PyTorch tensors ready for the model.\n",
    "\n",
    "Evaluation metrics: during evaluation, we want to evaluate the model using the word error rate (WER) metric. We need to define a compute_metrics function that handles this computation.\n",
    "\n",
    "Load a pre-trained checkpoint: we need to load a pre-trained checkpoint and configure it correctly for training.\n",
    "\n",
    "Define the training arguments: these will be used by the ğŸ¤— Trainer in constructing the training schedule.\n",
    "\n",
    "Once weâ€™ve fine-tuned the model, we will evaluate it on the test data to verify that we have correctly trained it to transcribe speech in Dhivehi.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(\n",
    "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [\n",
    "            {\"input_features\": feature[\"input_features\"][0]} for feature in features\n",
    "        ]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "```\n",
    "\n",
    "### 3.6.7 define Evaluation Metrics\n",
    "\n",
    "```python\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # ç”¨ pad_token_id æ›¿æ¢ -100\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # æˆ‘ä»¬å¸Œæœ›åœ¨è®¡ç®—æŒ‡æ ‡æ—¶ä¸è¦ç»„åˆèµ·è¯å…ƒ\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # è®¡ç®—æ™®é€šçš„ WER\n",
    "    wer_ortho = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    # è®¡ç®—æ ‡å‡†åŒ–çš„ WER\n",
    "    pred_str_norm = [normalizer(pred) for pred in pred_str]\n",
    "    label_str_norm = [normalizer(label) for label in label_str]\n",
    "    # è¿‡æ»¤ï¼Œä»è€Œåœ¨è¯„ä¼°æ—¶åªè®¡ç®— reference éç©ºçš„æ ·æœ¬\n",
    "    pred_str_norm = [\n",
    "        pred_str_norm[i] for i in range(len(pred_str_norm)) if len(label_str_norm[i]) > 0\n",
    "    ]\n",
    "    label_str_norm = [\n",
    "        label_str_norm[i]\n",
    "        for i in range(len(label_str_norm))\n",
    "        if len(label_str_norm[i]) > 0\n",
    "    ]\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str_norm, references=label_str_norm)\n",
    "\n",
    "    return {\"wer_ortho\": wer_ortho, \"wer\": wer}\n",
    "```\n",
    "\n",
    "### 3.6.8 Load a pre-trained checkpoint\n",
    "\n",
    "```python\n",
    "\n",
    "# 1 load checkpoint\n",
    "\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "```\n",
    "\n",
    "### 3.6.9 define the training arguments\n",
    "\n",
    "For more detail on the training arguments, refer to the Seq2SeqTrainingArguments [docs](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments).\n",
    "\n",
    "```python\n",
    "# some setting\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# train args: disable cache during training since it's incompatible with gradient checkpointing\n",
    "model.config.use_cache = False\n",
    "\n",
    "# generate args: ä¸ºç”Ÿæˆè®¾ç½®è¯­è¨€å’Œä»»åŠ¡ï¼Œå¹¶é‡æ–°å¯ç”¨ç¼“å­˜\n",
    "model.generate = partial(\n",
    "    model.generate, language=\"sinhalese\", task=\"transcribe\", use_cache=True\n",
    ")\n",
    "\n",
    "# train args\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-dv\",  # åœ¨ HF Hub ä¸Šçš„è¾“å‡ºç›®å½•çš„åå­—\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # æ¯æ¬¡ batch size ä¸‹è°ƒåˆ°ä¸€åŠå°±æŠŠè¿™ä¸ªå‚æ•°ä¸Šè°ƒåˆ°ä¸¤å€\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    warmup_steps=50,\n",
    "    max_steps=500,  # å¦‚æœæ‚¨æœ‰è‡ªå·±çš„ GPU æˆ–è€… Colab ä»˜è´¹è®¡åˆ’ï¼Œä¸Šè°ƒåˆ° 4000\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "# We can forward the training arguments to the ğŸ¤— Trainer along with our model, dataset, data collator and compute_metrics function:\n",
    "\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")\n",
    "```\n",
    "\n",
    "### 3.6.10 train\n",
    "\n",
    "```python\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "### 3.6.11 share model\n",
    "\n",
    "```python\n",
    "kwargs = {\n",
    "    \"dataset_tags\": \"mozilla-foundation/common_voice_13_0\",\n",
    "    \"dataset\": \"Common Voice 13\",  # è®­ç»ƒæ•°æ®é›†\n",
    "    \"language\": \"dv\",\n",
    "    \"model_name\": \"Whisper Small Dv - Sanchit Gandhi\",  # ç»™æ¨¡å‹èµ·ä¸ªâ€œæ¼‚äº®â€çš„åå­—\n",
    "    \"finetuned_from\": \"openai/whisper-small\",\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "}\n",
    "\n",
    "trainer.push_to_hub(**kwargs)\n",
    "```\n",
    "\n",
    "## 3.7 build app demo\n",
    "\n",
    "- [gradio app demo](https://huggingface.co/learn/audio-course/en/chapter5/demo)\n",
    "\n",
    "# 4 more doc\n",
    "\n",
    "* [Whisper Talk](https://www.youtube.com/live/fZMiD8sDzzg?feature=share) by Jong Wook Kim: a presentation on the Whisper model, explaining the motivation, architecture, training and results, delivered by Whisper author Jong Wook Kim\n",
    "* [End-to-End Speech Benchmark (ESB)](https://arxiv.org/abs/2210.13352): a paper that comprehensively argues for using the orthographic WER as opposed to the normalised WER for evaluating ASR systems and presents an accompanying benchmark\n",
    "* [Fine-Tuning Whisper for Multilingual ASR](https://huggingface.co/blog/fine-tune-whisper): an in-depth blog post that explains how the Whisper model works in more detail, and the pre- and post-processing steps involved with the feature extractor and tokenizer\n",
    "* [Fine-tuning MMS Adapter Models for Multi-Lingual ASR](https://huggingface.co/blog/mms_adapters): an end-to-end guide for fine-tuning Meta AI's new [MMS](https://ai.facebook.com/blog/multilingual-model-speech-recognition/) speech recognition models, freezing the base model weights and only fine-tuning a small number of *adapter* layers\n",
    "* [Boosting Wav2Vec2 with n-grams in ğŸ¤— Transformers](https://huggingface.co/blog/wav2vec2-with-ngram): a blog post for combining CTC models with external language models (LMs) to combat spelling and punctuation errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ec68e4-89a6-413f-8bde-ba14f06ea0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
